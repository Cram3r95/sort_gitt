{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Online Realtime Tracking (SORT)\n",
    "\n",
    "### Visión Artificial. GITT-GIEC\n",
    "#### Luis M. Bergasa, Carlos Gómez Huélamo. Department of Electronics. University of Alcalá. Spain\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this practice is to use SORT algorithm to track multiple objects across images in order to associate them with a unique identity. As detector we use YOLO algorithm than next to SORT will allow the correct tracking of the objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Before designing the system, the corresponding libraries to be used must be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robesafe/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # In order to avoid: RuntimeError: main thread is not in main loop exception\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from skimage import io\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "from filterpy.kalman import KalmanFilter\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SORT Foundations \n",
    "\n",
    "SORT is an algorithm to object tracking where classical approaches like Kalman filters and Hungarian methods are used to track objects better than many online trackers. SORT is made of 4 key components which are as follows:\n",
    "\n",
    "- **Detection**: This is the first step in the tracking module. In this step, an object detector detects the objects in the frame that are to be tracked. These detections are then passed on to the next step. In this practice we use YOLO (You Only Look Once) because it is an efficient state-of-the-art open source algorithm for real-time object detection, which makes use of a single convolutional neural network to detect objects in images.\n",
    "\n",
    "- **Estimation**: In this step, the detections are propagated from the current frame to the next, which is estimating the position of the target in the next frame using a constant velocity model. When a detection is associated with a target, the detected bounding box is used to update the target state where the velocity components are optimally solved via the Kalman filter framework.\n",
    "\n",
    "- **Data association**: We now have the target bounding box and the detected bounding box. So, a cost matrix is computed as the intersection-over-union (IOU) distance between each detection and all predicted bounding boxes from the existing targets. The assignment is solved optimally using the Hungarian algorithm. If the IOU of detection and target is less than a certain threshold value called IOUmin then that assignment is rejected. This technique solves the occlusion problem and helps maintain the IDs.\n",
    "\n",
    "- **Creation and Deletion of Track Identities**: This module is responsible for the creation and deletion of IDs. Unique identities are created and destroyed according to the `iou_Th`. If the overlap of detection and target is less than  `iou_Th` then it signifies the untracked object. Tracks are created when the number of consecutive detections are higher than `min_hits`. Tracks are terminated if they are not detected for `max_age` frames. If an object reappears after `max_age` frames, tracking will implicitly resume under a new identity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SORT without detection\n",
    "\n",
    "In this section we are going to focus only on tracking. To do this, the detections are given directly in a `det.txt` file found in the `\\det` directory of the `KITTI-17` folder.\n",
    "\n",
    "Taking into account that the state vector and the measurement vector of the tracker is:\n",
    "\n",
    "$$\\mathbf{x} = \n",
    "\\begin{bmatrix}x & y & s & r & \\dot{x}  & \\dot{y} & \\dot{s}\\end{bmatrix}^\\mathsf{T}$$\n",
    "$$\\mathbf{z} = \n",
    "\\begin{bmatrix}x & y & s & r\\end{bmatrix}^\\mathsf{T}$$\n",
    "\n",
    "\n",
    "1. Indicate the meaning of each of the components of the state vector\n",
    "\n",
    "2. Complete the state transition matrix **F** and the observation matrix **H**.\n",
    "\n",
    "3. Run the code shown below\n",
    "\n",
    "4. Obtain the value of the following matrices: **P**, **Q**, **R**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1566832622532,
     "user": {
      "displayName": "D g",
      "photoUrl": "",
      "userId": "13512637910240542281"
     },
     "user_tz": -120
    },
    "id": "iHWrTd_Kc0Wa",
    "outputId": "cde185fe-56c7-4130-9136-ba523cc889fb"
   },
   "outputs": [],
   "source": [
    "def linear_assignment(cost_matrix):\n",
    "  try:\n",
    "    import lap\n",
    "    _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n",
    "    return np.array([[y[i],i] for i in x if i >= 0]) #\n",
    "  except ImportError:\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    x, y = linear_sum_assignment(cost_matrix)\n",
    "    return np.array(list(zip(x, y)))\n",
    "\n",
    "\n",
    "def iou_batch(bb_test, bb_gt):\n",
    "  \"\"\"\n",
    "  From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n",
    "  \"\"\"\n",
    "  bb_gt = np.expand_dims(bb_gt, 0)\n",
    "  bb_test = np.expand_dims(bb_test, 1)\n",
    "  \n",
    "  xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n",
    "  yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n",
    "  xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n",
    "  yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n",
    "  w = np.maximum(0., xx2 - xx1)\n",
    "  h = np.maximum(0., yy2 - yy1)\n",
    "  wh = w * h\n",
    "  o = wh / ((bb_test[..., 2] - bb_test[..., 0]) * (bb_test[..., 3] - bb_test[..., 1])                                      \n",
    "    + (bb_gt[..., 2] - bb_gt[..., 0]) * (bb_gt[..., 3] - bb_gt[..., 1]) - wh)                                              \n",
    "  return(o)  \n",
    "\n",
    "\n",
    "def convert_bbox_to_z(bbox):\n",
    "  \"\"\"\n",
    "  Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n",
    "    [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n",
    "    the aspect ratio\n",
    "  \"\"\"\n",
    "  w = bbox[2] - bbox[0]\n",
    "  h = bbox[3] - bbox[1]\n",
    "  x = bbox[0] + w/2.\n",
    "  y = bbox[1] + h/2.\n",
    "  s = w * h    #scale is just area\n",
    "  r = w / float(h)\n",
    "  return np.array([x, y, s, r]).reshape((4, 1))\n",
    "\n",
    "\n",
    "def convert_x_to_bbox(x,score=None):\n",
    "  \"\"\"\n",
    "  Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n",
    "    [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n",
    "  \"\"\"\n",
    "  w = np.sqrt(x[2] * x[3])\n",
    "  h = x[2] / w\n",
    "  if(score==None):\n",
    "    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.]).reshape((1,4))\n",
    "  else:\n",
    "    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.,score]).reshape((1,5))\n",
    "\n",
    "\n",
    "class KalmanBoxTracker(object):\n",
    "  \"\"\"\n",
    "  This class represents the internal state of individual tracked objects observed as bbox.\n",
    "  \"\"\"\n",
    "  count = 0\n",
    "  def __init__(self,bbox):\n",
    "    \"\"\"\n",
    "    Initialises a tracker using initial bounding box.\n",
    "    \"\"\"\n",
    "    #define constant velocity model\n",
    "    self.kf = KalmanFilter(dim_x=7, dim_z=4) \n",
    "    self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\n",
    "    self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\n",
    "\n",
    "    self.kf.R[2:,2:] *= 10.\n",
    "    self.kf.P[4:,4:] *= 1000. #give high uncertainty to the unobservable initial velocities\n",
    "    self.kf.P *= 10.\n",
    "    self.kf.Q[-1,-1] *= 0.01\n",
    "    self.kf.Q[4:,4:] *= 0.01\n",
    "\n",
    "    self.kf.x[:4] = convert_bbox_to_z(bbox)\n",
    "    self.time_since_update = 0\n",
    "    self.id = KalmanBoxTracker.count\n",
    "    KalmanBoxTracker.count += 1\n",
    "    self.history = []\n",
    "    self.hits = 0\n",
    "    self.hit_streak = 0\n",
    "    self.age = 0\n",
    "\n",
    "  def update(self,bbox):\n",
    "    \"\"\"\n",
    "    Updates the state vector with observed bbox.\n",
    "    \"\"\"\n",
    "    self.time_since_update = 0\n",
    "    self.history = []\n",
    "    self.hits += 1\n",
    "    self.hit_streak += 1\n",
    "    self.kf.update(convert_bbox_to_z(bbox))\n",
    "\n",
    "  def predict(self):\n",
    "    \"\"\"\n",
    "    Advances the state vector and returns the predicted bounding box estimate.\n",
    "    \"\"\"\n",
    "    if((self.kf.x[6]+self.kf.x[2])<=0):\n",
    "      self.kf.x[6] *= 0.0\n",
    "    self.kf.predict()\n",
    "    self.age += 1\n",
    "    if(self.time_since_update>0):\n",
    "      self.hit_streak = 0\n",
    "    self.time_since_update += 1\n",
    "    self.history.append(convert_x_to_bbox(self.kf.x))\n",
    "    return self.history[-1]\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\"\n",
    "    Returns the current bounding box estimate.\n",
    "    \"\"\"\n",
    "    return convert_x_to_bbox(self.kf.x)\n",
    "\n",
    "\n",
    "def associate_detections_to_trackers(detections,trackers,frame,iou_threshold = 0.3):\n",
    "  \"\"\"\n",
    "  Assigns detections to tracked object (both represented as bounding boxes)\n",
    "\n",
    "  Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n",
    "  \"\"\"\n",
    "  if(len(trackers)==0):\n",
    "    return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n",
    "\n",
    "  iou_matrix = iou_batch(detections, trackers)\n",
    "  # Show the Association matrix for the frame 100\n",
    "  if frame == 100:\n",
    "      print(\"Print Association matrix: \", iou_matrix)\n",
    "\n",
    "  if min(iou_matrix.shape) > 0:\n",
    "    a = (iou_matrix > iou_threshold).astype(np.int32)\n",
    "    if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n",
    "        matched_indices = np.stack(np.where(a), axis=1)\n",
    "    else:\n",
    "      matched_indices = linear_assignment(-iou_matrix)\n",
    "  else:\n",
    "    matched_indices = np.empty(shape=(0,2))\n",
    "\n",
    "  unmatched_detections = []\n",
    "  for d, det in enumerate(detections):\n",
    "    if(d not in matched_indices[:,0]):\n",
    "      unmatched_detections.append(d)\n",
    "  unmatched_trackers = []\n",
    "  for t, trk in enumerate(trackers):\n",
    "    if(t not in matched_indices[:,1]):\n",
    "      unmatched_trackers.append(t)\n",
    "\n",
    "  #filter out matched with low IOU\n",
    "  matches = []\n",
    "  for m in matched_indices:\n",
    "    if(iou_matrix[m[0], m[1]]<iou_threshold):\n",
    "      unmatched_detections.append(m[0])\n",
    "      unmatched_trackers.append(m[1])\n",
    "    else:\n",
    "      matches.append(m.reshape(1,2))\n",
    "  if(len(matches)==0):\n",
    "    matches = np.empty((0,2),dtype=int)\n",
    "  else:\n",
    "    matches = np.concatenate(matches,axis=0)\n",
    "\n",
    "  return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n",
    "\n",
    "\n",
    "class Sort(object):\n",
    "  def __init__(self, max_age=1, min_hits=3, iou_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Sets key parameters for SORT\n",
    "    \"\"\"\n",
    "    self.max_age = max_age\n",
    "    self.min_hits = min_hits\n",
    "    self.iou_threshold = iou_threshold\n",
    "    self.trackers = []\n",
    "    self.frame_count = 0\n",
    "\n",
    "  def update(self, dets=np.empty((0, 5))):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "      dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n",
    "    Requires: this method must be called once for each frame even with empty detections (use np.empty((0, 5)) for frames without detections).\n",
    "    Returns the a similar array, where the last column is the object ID.\n",
    "\n",
    "    NOTE: The number of objects returned may differ from the number of detections provided.\n",
    "    \"\"\"\n",
    "    self.frame_count += 1\n",
    "    # get predicted locations from existing trackers.\n",
    "    trks = np.zeros((len(self.trackers), 5))\n",
    "    to_del = []\n",
    "    ret = []\n",
    "    for t, trk in enumerate(trks):\n",
    "      pos = self.trackers[t].predict()[0]\n",
    "      trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n",
    "      if np.any(np.isnan(pos)):\n",
    "        to_del.append(t)\n",
    "    trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n",
    "    for t in reversed(to_del):\n",
    "      self.trackers.pop(t)\n",
    "      \n",
    "    matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets, trks, self.frame_count, self.iou_threshold)\n",
    "\n",
    "    # update matched trackers with assigned detections\n",
    "    for m in matched:\n",
    "      self.trackers[m[1]].update(dets[m[0], :])\n",
    "\n",
    "    # create and initialise new trackers for unmatched detections\n",
    "    for i in unmatched_dets:\n",
    "        trk = KalmanBoxTracker(dets[i,:])\n",
    "        self.trackers.append(trk)\n",
    "    i = len(self.trackers)\n",
    "    for trk in reversed(self.trackers):\n",
    "        d = trk.get_state()[0]\n",
    "        if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n",
    "          ret.append(np.concatenate((d,[trk.id+1])).reshape(1,-1)) # +1 as MOT benchmark requires positive\n",
    "        i -= 1\n",
    "        # remove dead tracklet\n",
    "        if(trk.time_since_update > self.max_age):\n",
    "          self.trackers.pop(i)\n",
    "    if(len(ret)>0):\n",
    "      return np.concatenate(ret)\n",
    "    return np.empty((0,5))\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse input arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='SORT demo')\n",
    "    parser.add_argument('--display', dest='display', help='Display online tracker output (slow) [False]',action='store_true', default=True)\n",
    "    parser.add_argument(\"--seq_path\", help=\"Path to detections.\", type=str, default='mot_benchmark')\n",
    "    parser.add_argument(\"--phase\", help=\"Subdirectory in seq_path.\", type=str, default='train')\n",
    "    parser.add_argument(\"--max_age\", \n",
    "                        help=\"Maximum number of frames to keep alive a track without associated detections.\", \n",
    "                        type=int, default=5)\n",
    "    parser.add_argument(\"--min_hits\", \n",
    "                        help=\"Minimum number of associated detections before track is initialised.\", \n",
    "                        type=int, default=3)\n",
    "    parser.add_argument(\"--iou_threshold\", help=\"Minimum IOU for match.\", type=float, default=0.3)\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing KITTI-17\n",
      "Processing KITTI-17.\n",
      "Files remaining:  145\n",
      "Files remaining:  125\n",
      "Files remaining:  105\n",
      "Files remaining:  85\n",
      "Files remaining:  65\n",
      "Print Association matrix:  [[   0.069304     0.79653           0           0           0]\n",
      " [          0           0     0.87982           0           0]\n",
      " [          0           0           0     0.89269           0]\n",
      " [    0.83658     0.04047           0           0           0]]\n",
      "Files remaining:  45\n",
      "Files remaining:  25\n",
      "Files remaining:  5\n",
      "Total Tracking took: 0.147 seconds for 145 frames or 983.3 FPS\n",
      "Note: to get real runtime results run without the option: --display\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "SAVE_VIDEO = True # If True, save the qualitative results as a video file. Otherwise, plot in real-time\n",
    "USE_OWN_DETECTIONS = False # Control flaw for using our own objects detector\n",
    "############################################################################################\n",
    "\n",
    "args = parse_args()\n",
    "display = args.display\n",
    "phase = args.phase\n",
    "total_time = 0.0\n",
    "total_frames = 0\n",
    "colours = np.random.rand(32, 3) #used only for display\n",
    "\n",
    "output_resolution = (864,576)\n",
    "\n",
    "if(display):\n",
    "  if not os.path.exists('mot_benchmark'):\n",
    "    print('\\n\\tERROR: mot_benchmark link not found!\\n\\n    Create a symbolic link to the MOT benchmark\\n    (https://motchallenge.net/data/2D_MOT_2015/#download). E.g.:\\n\\n    $ ln -s /path/to/MOT2015_challenge/2DMOT2015 mot_benchmark\\n\\n')\n",
    "    exit()\n",
    "  plt.ion()\n",
    "  fig = plt.figure()\n",
    "  ax1 = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "if not os.path.exists('output'):\n",
    "  os.makedirs('output')\n",
    "pattern = os.path.join(args.seq_path, phase, '*', 'det', 'det.txt')\n",
    "\n",
    "# Initialize detector if required\n",
    "\n",
    "if USE_OWN_DETECTIONS:\n",
    "  detection_model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "  \n",
    "for seq_dets_fn in glob.glob(pattern):\n",
    "  if SAVE_VIDEO:\n",
    "    if USE_OWN_DETECTIONS: detections = \"YOLO-detections\"\n",
    "    else: detections = \"default-detections\"\n",
    "    video_writer = cv2.VideoWriter(f'output/KITTI-17-qualitative-results-{detections}.avi',cv2.VideoWriter_fourcc(*'XVID'),20.0,output_resolution)\n",
    "    \n",
    "  # Call to the tracker\n",
    "  mot_tracker = Sort(max_age=args.max_age, \n",
    "                      min_hits=args.min_hits,\n",
    "                      iou_threshold=args.iou_threshold) #create instance of the SORT tracker\n",
    "  seq_dets = np.loadtxt(seq_dets_fn, delimiter=',')\n",
    "  seq = seq_dets_fn[pattern.find('*'):].split(os.path.sep)[0]\n",
    "\n",
    "  print(f\"Analyzing {seq}\")\n",
    "\n",
    "  with open(os.path.join('output', '%s.txt'%(seq)),'w') as out_file:\n",
    "    print(\"Processing %s.\"%(seq))\n",
    "    for frame in range(int(seq_dets[:,0].max())):\n",
    "      if frame % 20 == 0:\n",
    "        print(\"Files remaining: \", int(seq_dets[:,0].max()) - frame)\n",
    "\n",
    "      frame += 1 #detection and frame numbers begin at 1\n",
    "      if not USE_OWN_DETECTIONS: # Detections come from a file\n",
    "        dets = seq_dets[seq_dets[:, 0]==frame, 2:7]\n",
    "        dets[:, 2:4] += dets[:, 0:2] #convert to [x1,y1,w,h] to [x1,y1,x2,y2]\n",
    "      total_frames += 1\n",
    "\n",
    "      if(display):\n",
    "        fn = os.path.join('mot_benchmark', phase, seq, 'img1', '%06d.jpg'%(frame))\n",
    "        im =io.imread(fn)\n",
    "        ax1.imshow(im)\n",
    "        plt.title(seq + ' Tracked Targets')\n",
    "\n",
    "      if USE_OWN_DETECTIONS: # Detections come from YOLO\n",
    "        detection_results = detection_model(fn, verbose=False)[0] # 0 since batch_size = 1 in this case\n",
    "        \n",
    "        # OBS: Set verbose to True if you want to activate the detector output info\n",
    "        # detection_results is an object derived from the class 'ultralytics.yolo.engine.results.Results'\n",
    "        # Attributes:\n",
    "          # orig_img (numpy.ndarray): The original image as a numpy array.\n",
    "          # orig_shape (tuple): The original image shape in (height, width) format.\n",
    "          # boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n",
    "          # masks (Masks, optional): A Masks object containing the detection masks.\n",
    "          # probs (numpy.ndarray, optional): A 2D numpy array of detection probabilities for each class.\n",
    "          # names (dict): A dictionary of class names.\n",
    "          # path (str): The path to the image file.\n",
    "          # keypoints (List[List[float]], optional): A list of detected keypoints for each object.\n",
    "          # speed (dict): A dictionary of preprocess, inference and postprocess speeds in milliseconds per image.\n",
    "          # _keys (tuple): A tuple of attribute names for non-empty attributes.\n",
    "          \n",
    "        # 1. Get coordinates (boxes have different formats: xywh, xywhn, xyxy, xyxyn (n means normalized)). \n",
    "        # Since in this lesson we require the xy coordinates of the top-left and bottom-right corner -> xyxy\n",
    "\n",
    "        dets = detection_results.boxes.xyxy.cpu().detach().numpy()\n",
    "        \n",
    "        # 2. In this lesson we will only track pedestrians. To check the classes of our DL model (YOLOv8 with this configuration):\n",
    "        \n",
    "        # detection_results.names -> You can check 0 is person\n",
    "        \n",
    "        # 2.1. Get object types:\n",
    "        \n",
    "        object_types = detection_results.boxes.cls.cpu().detach().numpy()\n",
    "        \n",
    "        # 2.2. Filter 2D bounding boxes coordinates according to our class of interest (Pedestrian == 0)\n",
    "        \n",
    "        type_of_interest_id = 0 # Introduce Pedestrian ID\n",
    "        dets = dets[object_types == type_of_interest_id,:] # Chose the files corresponding with Pedestrians\n",
    "\n",
    "      start_time = time.time()\n",
    "      trackers = mot_tracker.update(dets)\n",
    "      cycle_time = time.time() - start_time\n",
    "      total_time += cycle_time\n",
    "\n",
    "      for d in trackers:\n",
    "        print('%d,%d,%.2f,%.2f,%.2f,%.2f,1,-1,-1,-1'%(frame,d[4],d[0],d[1],d[2]-d[0],d[3]-d[1]),file=out_file)\n",
    "        if(display):\n",
    "          d = d.astype(np.int32)\n",
    "          tracker_colour = colours[d[4]%32,:]\n",
    "          ax1.add_patch(patches.Rectangle((d[0],d[1]),d[2]-d[0],d[3]-d[1],fill=False,lw=3,ec=tracker_colour))\n",
    "          label = 'ID %0d'%d[4].astype(int) # Generate a label with the tracker ID\n",
    "          ax1.text(d[0]+20,d[1],label,color=tracker_colour) # Show the label for each box using ax1.text(). Hint: use tracker_color\n",
    "\n",
    "      fig.canvas.draw()\n",
    "\n",
    "      # Now we can save it to a numpy array.\n",
    "      data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "      data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "      \n",
    "      if display:\n",
    "        if SAVE_VIDEO:\n",
    "          data = cv2.resize(data,output_resolution, interpolation = cv2.INTER_AREA)\n",
    "          video_writer.write(data)\n",
    "        else:\n",
    "          data1 = cv2.resize(data,output_resolution, interpolation = cv2.INTER_AREA) # To see better the image\n",
    "          cv2.imshow(\"Output Image\", data1)\n",
    "          ax1.cla()\n",
    "          \n",
    "          # Exit if ESC pressed\n",
    "          k = cv2.waitKey(1) & 0xff\n",
    "          if k == 27 : break\n",
    "          \n",
    "        fig.canvas.flush_events()\n",
    "        plt.draw()\n",
    "        ax1.cla()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Total Tracking took: %.3f seconds for %d frames or %.1f FPS\" % (total_time, total_frames, total_frames / total_time))\n",
    "\n",
    "if(display):\n",
    "  print(\"Note: to get real runtime results run without the option: --display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Questions\n",
    "\n",
    "**1-Which value of dt should be selected in this type of system for the Kalman fi\n",
    "\n",
    "Run the code shown below, where the following state vector has been used:\n",
    "\n",
    "\n",
    "Specify default values for iou_th, max_age and min_hits\n",
    "\n",
    "Modify the value of iou_th = 0.7 and observe the results obtained\n",
    "Modify the value of min_hits = 10 and observe the results obtained\n",
    "Modify the value of max_age = 10 and observe the results obtained\n",
    "Add a text to each box that includes the tracker identifier number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv8 implementation\n",
    "\n",
    "The following code cell contains the heart of YOLO, meaning that it includes both the YOLO feature extractor and its base class, which in turn defines the initialisation, weight loading and prediction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1566832623269,
     "user": {
      "displayName": "D g",
      "photoUrl": "",
      "userId": "13512637910240542281"
     },
     "user_tz": -120
    },
    "id": "NlGRyoAYdClX",
    "outputId": "61b2aa3a-9be4-479e-fb69-52747dae0005"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "New https://pypi.org/project/ultralytics/8.0.82 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.81 🚀 Python-3.8.10 torch-1.13.1+cu117 CUDA:0 (A100-SXM4-80GB, 81252MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/robesafe/sort_gitt/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/robesafe/sort_gitt/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3      2.54G      1.182      1.408      1.213        218        640: 100%|██████████| 8/8 [00:00<00:00,  8.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00<00:00,  6.04it/s]\n",
      "                   all        128        929      0.635      0.561      0.625      0.463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3      2.56G      1.136      1.318      1.239        205        640: 100%|██████████| 8/8 [00:00<00:00, 11.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00<00:00,  5.85it/s]\n",
      "                   all        128        929      0.665      0.546      0.634      0.469\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3      2.51G      1.172      1.293      1.219        161        640: 100%|██████████| 8/8 [00:00<00:00, 12.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.24it/s]\n",
      "                   all        128        929      0.669      0.597      0.664      0.495\n",
      "\n",
      "3 epochs completed in 0.003 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.5MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.5MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.81 🚀 Python-3.8.10 torch-1.13.1+cu117 CUDA:0 (A100-SXM4-80GB, 81252MiB)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.33it/s]\n",
      "                   all        128        929      0.669      0.601      0.665      0.496\n",
      "                person        128        254      0.811      0.681      0.762       0.55\n",
      "               bicycle        128          6      0.598      0.265      0.328      0.259\n",
      "                   car        128         46      0.797      0.217      0.331      0.192\n",
      "            motorcycle        128          5      0.683      0.869      0.938      0.747\n",
      "              airplane        128          6      0.784      0.833      0.903      0.689\n",
      "                   bus        128          7      0.751      0.714      0.722      0.661\n",
      "                 train        128          3      0.731      0.922      0.913       0.83\n",
      "                 truck        128         12          1      0.319      0.521      0.341\n",
      "                  boat        128          6      0.302      0.225      0.385      0.246\n",
      "         traffic light        128         14       0.73      0.197      0.206      0.139\n",
      "             stop sign        128          2      0.765          1      0.995      0.647\n",
      "                 bench        128          9      0.843      0.602       0.66      0.421\n",
      "                  bird        128         16      0.937      0.924      0.969       0.65\n",
      "                   cat        128          4      0.977          1      0.995      0.843\n",
      "                   dog        128          9      0.576      0.889      0.896      0.695\n",
      "                 horse        128          2      0.625          1      0.995      0.597\n",
      "              elephant        128         17      0.832      0.882      0.935      0.743\n",
      "                  bear        128          1      0.541          1      0.995      0.995\n",
      "                 zebra        128          4      0.862          1      0.995      0.934\n",
      "               giraffe        128          9      0.815      0.889      0.955      0.674\n",
      "              backpack        128          6      0.456      0.333      0.344      0.208\n",
      "              umbrella        128         18      0.721      0.556       0.68      0.468\n",
      "               handbag        128         19      0.841      0.105      0.262      0.144\n",
      "                   tie        128          7      0.763      0.714      0.787      0.578\n",
      "              suitcase        128          4      0.571          1      0.788      0.543\n",
      "               frisbee        128          5      0.583        0.8      0.732      0.655\n",
      "                  skis        128          1      0.585          1      0.995      0.512\n",
      "             snowboard        128          7      0.472      0.714      0.758      0.464\n",
      "           sports ball        128          6      0.734      0.468      0.573      0.308\n",
      "                  kite        128         10      0.737      0.565      0.596      0.199\n",
      "          baseball bat        128          4      0.459       0.25      0.373      0.199\n",
      "        baseball glove        128          7      0.795      0.429       0.43      0.302\n",
      "            skateboard        128          5      0.769        0.6        0.6      0.446\n",
      "         tennis racket        128          7      0.707      0.355      0.506      0.328\n",
      "                bottle        128         18      0.365       0.48      0.429      0.249\n",
      "            wine glass        128         16      0.581        0.5      0.626      0.366\n",
      "                   cup        128         36      0.689      0.333      0.434      0.293\n",
      "                  fork        128          6      0.586      0.167      0.288      0.223\n",
      "                 knife        128         16      0.583      0.625      0.643       0.38\n",
      "                 spoon        128         22      0.623      0.227      0.342      0.204\n",
      "                  bowl        128         28       0.68      0.607      0.653      0.514\n",
      "                banana        128          1     0.0595      0.297      0.199      0.048\n",
      "              sandwich        128          2          1      0.875      0.995      0.995\n",
      "                orange        128          4          1      0.458      0.995      0.705\n",
      "              broccoli        128         11        0.5      0.182      0.282      0.243\n",
      "                carrot        128         24      0.583      0.792      0.768      0.477\n",
      "               hot dog        128          2      0.558          1      0.995      0.945\n",
      "                 pizza        128          5      0.772          1      0.995       0.83\n",
      "                 donut        128         14      0.642          1      0.928      0.838\n",
      "                  cake        128          4      0.813          1      0.995      0.904\n",
      "                 chair        128         35      0.418      0.457       0.41      0.257\n",
      "                 couch        128          6      0.557        0.5      0.705      0.516\n",
      "          potted plant        128         14      0.527      0.717      0.698      0.479\n",
      "                   bed        128          3      0.827          1      0.995      0.779\n",
      "          dining table        128         13      0.471      0.615      0.512      0.418\n",
      "                toilet        128          2      0.636        0.5      0.828      0.745\n",
      "                    tv        128          2      0.435        0.5      0.745      0.696\n",
      "                laptop        128          3          1      0.576      0.727      0.611\n",
      "                 mouse        128          2          1          0     0.0362    0.00724\n",
      "                remote        128          8      0.723        0.5      0.574      0.499\n",
      "            cell phone        128          8          0          0     0.0991     0.0437\n",
      "             microwave        128          3      0.732          1       0.83      0.693\n",
      "                  oven        128          5      0.505        0.4      0.434      0.321\n",
      "                  sink        128          6       0.32      0.167      0.427        0.2\n",
      "          refrigerator        128          5      0.754        0.8      0.846      0.703\n",
      "                  book        128         29       0.54      0.163      0.336       0.17\n",
      "                 clock        128          9      0.862      0.889      0.918      0.775\n",
      "                  vase        128          2      0.374          1      0.995      0.895\n",
      "              scissors        128          1          1          0      0.249     0.0746\n",
      "            teddy bear        128         21      0.662      0.429       0.66      0.428\n",
      "            toothbrush        128          5          1      0.571        0.8      0.491\n",
      "Speed: 0.7ms preprocess, 0.5ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Ultralytics YOLOv8.0.81 🚀 Python-3.8.10 torch-1.13.1+cu117 CUDA:0 (A100-SXM4-80GB, 81252MiB)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/robesafe/sort_gitt/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:02<00:00,  3.54it/s]\n",
      "                   all        128        929      0.695      0.566      0.659      0.491\n",
      "                person        128        254      0.865      0.657      0.775      0.555\n",
      "               bicycle        128          6      0.524      0.167       0.32      0.253\n",
      "                   car        128         46      0.861      0.217      0.333      0.191\n",
      "            motorcycle        128          5      0.674      0.833      0.938      0.747\n",
      "              airplane        128          6      0.812      0.833      0.903      0.673\n",
      "                   bus        128          7      0.821      0.714      0.722      0.656\n",
      "                 train        128          3      0.722      0.887      0.913       0.83\n",
      "                 truck        128         12          1      0.301       0.51       0.33\n",
      "                  boat        128          6        0.3      0.167      0.341       0.17\n",
      "         traffic light        128         14      0.684      0.157      0.207      0.133\n",
      "             stop sign        128          2      0.805          1      0.995      0.647\n",
      "                 bench        128          9      0.824      0.523       0.66      0.423\n",
      "                  bird        128         16      0.935        0.9      0.969      0.656\n",
      "                   cat        128          4          1      0.991      0.995      0.843\n",
      "                   dog        128          9      0.599      0.889      0.896      0.695\n",
      "                 horse        128          2       0.65          1      0.995      0.597\n",
      "              elephant        128         17      0.892      0.882      0.935      0.746\n",
      "                  bear        128          1      0.572          1      0.995      0.995\n",
      "                 zebra        128          4      0.872          1      0.995      0.965\n",
      "               giraffe        128          9      0.895      0.951      0.984      0.716\n",
      "              backpack        128          6      0.474      0.333      0.335      0.188\n",
      "              umbrella        128         18      0.756      0.556      0.677      0.462\n",
      "               handbag        128         19          1     0.0997      0.258      0.144\n",
      "                   tie        128          7      0.787      0.714      0.787      0.562\n",
      "              suitcase        128          4      0.552       0.75      0.788      0.543\n",
      "               frisbee        128          5      0.599        0.8      0.732      0.639\n",
      "                  skis        128          1      0.766          1      0.995      0.514\n",
      "             snowboard        128          7      0.601      0.714      0.758      0.464\n",
      "           sports ball        128          6      0.711      0.422      0.544      0.294\n",
      "                  kite        128         10      0.708      0.487      0.602      0.199\n",
      "          baseball bat        128          4      0.499       0.25      0.348      0.199\n",
      "        baseball glove        128          7      0.759      0.429       0.43      0.303\n",
      "            skateboard        128          5      0.905        0.6      0.606      0.444\n",
      "         tennis racket        128          7      0.682      0.313      0.506      0.345\n",
      "                bottle        128         18      0.398      0.405      0.435      0.272\n",
      "            wine glass        128         16      0.571      0.417      0.593      0.359\n",
      "                   cup        128         36      0.723      0.333      0.457      0.311\n",
      "                  fork        128          6      0.603      0.167      0.228      0.198\n",
      "                 knife        128         16      0.618      0.607       0.63      0.368\n",
      "                 spoon        128         22      0.827      0.219      0.365      0.218\n",
      "                  bowl        128         28      0.764      0.679      0.682      0.536\n",
      "                banana        128          1          0          0      0.124     0.0364\n",
      "              sandwich        128          2      0.852        0.5      0.828      0.828\n",
      "                orange        128          4          1      0.387      0.995      0.662\n",
      "              broccoli        128         11       0.44      0.182       0.28      0.242\n",
      "                carrot        128         24      0.663      0.819      0.768      0.484\n",
      "               hot dog        128          2      0.571          1      0.828      0.828\n",
      "                 pizza        128          5          1      0.988      0.995      0.837\n",
      "                 donut        128         14      0.655          1      0.928      0.823\n",
      "                  cake        128          4       0.71          1      0.995      0.904\n",
      "                 chair        128         35      0.403        0.4      0.406      0.256\n",
      "                 couch        128          6      0.659        0.5      0.768      0.578\n",
      "          potted plant        128         14      0.572      0.643      0.699      0.481\n",
      "                   bed        128          3       0.82          1      0.995      0.764\n",
      "          dining table        128         13      0.471      0.462      0.479      0.395\n",
      "                toilet        128          2      0.663        0.5      0.828      0.745\n",
      "                    tv        128          2      0.468        0.5      0.745      0.696\n",
      "                laptop        128          3          1      0.545      0.703      0.594\n",
      "                 mouse        128          2          1          0     0.0382    0.00764\n",
      "                remote        128          8      0.785        0.5      0.586      0.502\n",
      "            cell phone        128          8          0          0     0.0906      0.042\n",
      "             microwave        128          3      0.602      0.537       0.83       0.65\n",
      "                  oven        128          5      0.525        0.4      0.435      0.321\n",
      "                  sink        128          6      0.408      0.167      0.358      0.185\n",
      "          refrigerator        128          5          1      0.716      0.886      0.719\n",
      "                  book        128         29      0.498      0.138      0.358      0.179\n",
      "                 clock        128          9      0.916      0.889      0.917       0.76\n",
      "                  vase        128          2      0.404          1      0.995      0.895\n",
      "              scissors        128          1          1          0      0.332     0.0995\n",
      "            teddy bear        128         21       0.72      0.429      0.655      0.425\n",
      "            toothbrush        128          5      0.962        0.6      0.838      0.505\n",
      "Speed: 0.6ms preprocess, 1.4ms inference, 0.0ms loss, 5.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n",
      "\n",
      "image 1/1 /home/robesafe/sort_gitt/mot_benchmark/train/KITTI-17/img1/000001.jpg: 224x640 4 persons, 3 bicycles, 1 bus, 1 truck, 11.2ms\n",
      "Speed: 0.2ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "model.train(data=\"coco128.yaml\", epochs=3)  # train the model\n",
    "metrics = model.val()  # evaluate model performance on the validation set\n",
    "# results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
    "# success = model.export(format=\"onnx\")  # export the model to ONNX format\n",
    "# results = model(\"mot_benchmark/train/KITTI-17/img1/000001.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SORT with YOLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the control flaw USE_OWN_DETECTIONS to True in the previous code and compare the obtained results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "yolo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
